{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flare predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this prediction problem, we'll then ascribe each active region to one of two classes:\n",
    "\n",
    "1. The active region will produce a flare (positive class). \n",
    "\n",
    "2. The active region will not produce a flare (negative class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our CME predictor we need to gather our training dataset: \n",
    "\n",
    "1. Flare data from the GOES flare catalog at NOAA, which can be accessed with the `sunkit_instruments` library, a SunPy affiliated package for solar instrument-specific tools. This tells us if an active region produced a flare or not.\n",
    "\n",
    "2. Active region data from the Solar Dynamics Observatory's `Helioseismic and Magnetic Imager instrument` (HMI), which can be accessed from the [JSOC database](http://jsoc.stanford.edu/) via a JSON API. This gives us the features characterizing each active region `SHARP` â€“ Space-Weather HMI Active Region Patches [Bobra et al., 2014](http://link.springer.com/article/10.1007%2Fs11207-014-0529-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Gathering data for the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "from datetime import datetime as dt_obj\n",
    "from datetime import timedelta\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Sunpy\n",
    "try:\n",
    "    import sunpy\n",
    "    from sunpy.time import TimeRange\n",
    "except ImportError:\n",
    "    %pip install sunpy\n",
    "    from sunpy.time import TimeRange\n",
    "\n",
    "    \n",
    "# Sunkit-instruments\n",
    "try:\n",
    "    import sunkit_instruments\n",
    "except ImportError:\n",
    "    %pip install sunkit_instruments\n",
    "    import sunkit_instruments\n",
    "    \n",
    "\n",
    "# Shap values\n",
    "try: \n",
    "    import shap\n",
    "except:\n",
    "    %pip install shap\n",
    "    import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we grab all the data from the GOES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = \"2010-05-01\" # Using solar cycle 24\n",
    "t_end = \"2011-05-01\"  # Date for example, but using until 2024 for the full dataset (takes ~30min)\n",
    "\n",
    "flag_for_fulldataset = False # If True, will save the full dataset to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab all the data from the GOES database\n",
    "time_range = TimeRange(t_start, t_end)\n",
    "import sunkit_instruments.goes_xrs\n",
    "listofflares = sunkit_instruments.goes_xrs.get_goes_event_list(time_range, 'M1')\n",
    "print('Grabbed all the GOES data; there are', len(listofflares), 'events.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only keep the events that have a NOAA active region\n",
    "mistakes = 0\n",
    "toremove = []\n",
    "for i in range(len(listofflares)):\n",
    "    \n",
    "    if (listofflares[i]['noaa_active_region'] == 0):\n",
    "        # remove the event from the list\n",
    "        toremove.append(i)\n",
    "        mistakes += 1\n",
    "    \n",
    "    if (listofflares[i]['noaa_active_region'] < 11000):\n",
    "        # remove the event from the list\n",
    "        toremove.append(i)\n",
    "        mistakes += 1\n",
    "\n",
    "print('There are', mistakes, 'mistakes so far.')\n",
    "listofflares = [i for j, i in enumerate(listofflares) if j not in toremove]\n",
    "print('There are', len(listofflares), 'events left.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform this into a pandas dataframe\n",
    "df = pd.DataFrame(listofflares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will gather the file with the conversion between the name of the active region and the identification number given by the HMI@SDO database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.read_csv(\n",
    "    'http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelayvariable = 24 # in hours, feel free to produce a new dataset with a different time delay and check the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll convert subtract `timedelayvariable` from the GOES Peak Time and re-format the datetime object into a string that JSOC can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['peak_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rec = [(df['peak_time'].iloc[i] - timedelta(hours=timedelayvariable)\n",
    "          ).strftime('%Y.%m.%d_%H:%M_TAI') for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the SDO data from the JSOC database by executing the JSON queries. The SHARP parameters are calculated every 12 minutes during an AR lifetime. We are selecting data that satisfies several criteria: The data has to be [1] disambiguated with a version of the disambiguation module greater than 1.1, [2] taken while the orbital velocity of the spacecraft is less than 3500 m/s, [3] of a high quality, and [4] within 70 degrees of central meridian. If the data pass all these tests, they are put into one of two lists: one for the positive class (called CME_data) and one for the negative class (called no_CME_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_jsoc_data(event_count, t_rec):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_count: number of events \n",
    "                 int\n",
    "\n",
    "    t_rec:       list of times, one associated with each event in event_count\n",
    "                 list of strings in JSOC format ('%Y.%m.%d_%H:%M_TAI')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    catalog_data = []\n",
    "    classification = []\n",
    "\n",
    "    for i in range(event_count):\n",
    "\n",
    "        print(\"=====\", i,\"/\",event_count-1,\"=====\")\n",
    "        \n",
    "        # Check if the active region number is not zero:\n",
    "        if int(listofactiveregions[i]) == 0:\n",
    "            print('skip: NOAA Active Region number is zero')\n",
    "            continue\n",
    "        \n",
    "        # next match NOAA_ARS to HARPNUM\n",
    "        idx = answer[answer['NOAA_ARS'].str.contains(\n",
    "            str(int(listofactiveregions[i])))]\n",
    "\n",
    "        # if there's no HARPNUM, quit\n",
    "        if (idx.empty == True):\n",
    "            print('skip: there are no matching HARPNUMs for',\n",
    "                  str(int(listofactiveregions[i])))\n",
    "            continue\n",
    "\n",
    "        # construct jsoc_info queries and query jsoc database; we are querying for 25 keywords\n",
    "        url = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_720s[\"+str(\n",
    "            idx.HARPNUM.values[0])+\"][\"+t_rec[i]+\"][? (CODEVER7 !~ '1.1 ') and (abs(OBS_VR)< 3500) and (QUALITY<65536) ?]&op=rs_list&key=USFLUX,MEANGBT,MEANJZH,MEANPOT,SHRGT45,TOTUSJH,MEANGBH,MEANALP,MEANGAM,MEANGBZ,MEANJZD,TOTUSJZ,SAVNCPP,TOTPOT,MEANSHR,AREA_ACR,R_VALUE,ABSNJZH\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if there's no response at this time, quit\n",
    "        if response.status_code != 200:\n",
    "            print('skip: cannot successfully get an http response')\n",
    "            continue\n",
    "\n",
    "        # read the JSON output\n",
    "        data = response.json()\n",
    "\n",
    "        # if there are no data at this time, quit\n",
    "        if data['count'] == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        # check to see if the active region is too close to the limb\n",
    "        # we can compute the latitude of an active region in stonyhurst coordinates as follows:\n",
    "        # latitude_stonyhurst = CRVAL1 - CRLN_OBS\n",
    "        # for this we have to query the CEA series (but above we queried the other series as the CEA series does not have CODEVER5 in it)\n",
    "\n",
    "        url = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_cea_720s[\"+str(\n",
    "            idx.HARPNUM.values[0])+\"][\"+t_rec[i]+\"][? (abs(OBS_VR)< 3500) and (QUALITY<65536) ?]&op=rs_list&key=CRVAL1,CRLN_OBS\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if there's no response at this time, quit\n",
    "        if response.status_code != 200:\n",
    "            print('skip: failed to find CEA JSOC data for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        # read the JSON output\n",
    "        latitude_information = response.json()\n",
    "\n",
    "        # if there are no data at this time, quit\n",
    "        if latitude_information['count'] == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        CRVAL1 = float(latitude_information['keywords'][0]['values'][0])\n",
    "        CRLN_OBS = float(latitude_information['keywords'][1]['values'][0])\n",
    "        if (np.absolute(CRVAL1 - CRLN_OBS) > 70.0):\n",
    "            print('skip: latitude is out of range for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        if ('MISSING' in str(data['keywords'])):\n",
    "            print('skip: there are some missing keywords for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        print('accept NOAA Active Region number', str(int(\n",
    "            listofactiveregions[i])), 'and HARPNUM', idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "\n",
    "        individual_flare_data = []\n",
    "        for j in range(18):\n",
    "            individual_flare_data.append(\n",
    "                float(data['keywords'][j]['values'][0]))\n",
    "\n",
    "        catalog_data.append(list(individual_flare_data))\n",
    "\n",
    "        single_class_instance = [idx.HARPNUM.values[0], str(\n",
    "            int(listofactiveregions[i])), listofgoesclasses[i], t_rec[i]]\n",
    "        classification.append(single_class_instance)\n",
    "\n",
    "    return catalog_data, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data to be fed into the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofactiveregions = list(df['noaa_active_region'].values.flatten())\n",
    "listofgoesclasses = list(df['goes_class'].values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I am not in the github repo I need to download the data:\n",
    "try:\n",
    "    Flare_data = np.load('flare_cycle24+/Flare_data_full.npy')\n",
    "\n",
    "except:\n",
    "    !git clone https://github.com/cdiazbas/ESPD2024\n",
    "    import os\n",
    "    os.chdir('ESPD2024/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and save the data\n",
    "if flag_for_fulldataset:\n",
    "    Flare_data, Flare_class = get_the_jsoc_data(df.shape[0], t_rec)\n",
    "    np.save('Flare_data_full.npy', Flare_data)\n",
    "    np.save('Flare_class_full.npy', Flare_class)\n",
    "\n",
    "else:\n",
    "    Flare_data = np.load('flare_cycle24+/Flare_data_full.npy')\n",
    "    Flare_class = np.load('flare_cycle24+/Flare_class_full.npy')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the number of events associated with the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(Flare_data), \"flare events in the positive class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Gathering data for the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will define non-flare events as regions with:\n",
    "# -1, 0, + 1  AR region number (if same region, should be during quiet times: before the first flare and after the last flare)\n",
    "# -36, +12 hours from the peak time of the flare event\n",
    "n_negative_samples = len(Flare_data)*3\n",
    "\n",
    "random_time = [-36, 12]\n",
    "random_noaa = [-1,0,1]\n",
    "\n",
    "flaringAR = list(df['noaa_active_region'].values.flatten())\n",
    "flaringtime = list(df['peak_time'].values.flatten())\n",
    "listofactiveregions = []\n",
    "listofgoesclasses = []\n",
    "t_rec = []\n",
    "\n",
    "verbose = False\n",
    "for nn in range(n_negative_samples):\n",
    "    take1region = np.random.choice(flaringAR)\n",
    "    takeflaringtime = flaringtime[flaringAR.index(take1region)]\n",
    "    \n",
    "    newtime = int(np.random.choice(random_time))\n",
    "    newregion = -1 if newtime < 0 else 1\n",
    "    active_region = + newregion + take1region\n",
    "    time = takeflaringtime + timedelta(hours=newtime)\n",
    "    \n",
    "    if verbose:\n",
    "        print('flaring region based on:', take1region, 'flaring time:', takeflaringtime)\n",
    "        print('new region:', active_region, 'new time:', time)\n",
    "    \n",
    "    if active_region in flaringAR:\n",
    "\n",
    "        # Show all the peak times for the active region:\n",
    "        allflares_here = [flaringtime[i] for i in range(len(flaringAR)) if flaringAR[i] == active_region]\n",
    "        \n",
    "        # New time should be before 36 of the first peak of 12 after the last peak:\n",
    "        if (time - timedelta(hours=36) < allflares_here[0]) or (time + timedelta(hours=12) > allflares_here[-1]):\n",
    "            # print('skip: flaring time is too close to the peak times of the active region')\n",
    "            continue\n",
    "\n",
    "        \n",
    "    listofactiveregions.append(active_region)\n",
    "    listofgoesclasses.append('None')\n",
    "    t_rec.append((time).strftime('%Y.%m.%d_%H:%M_TAI'))\n",
    "\n",
    "n_negative_samples = len(listofactiveregions)\n",
    "\n",
    "print(\"There are\", len(listofactiveregions), \"non-flare events in the negative class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we compute times that are one day before the flare peak time and convert it into a string that JSOC can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and save the data\n",
    "if flag_for_fulldataset:\n",
    "    nonFlare_data, nonFlare_class = get_the_jsoc_data(n_negative_samples, t_rec)\n",
    "\n",
    "    np.save('nonFlare_data_full.npy', nonFlare_data)\n",
    "    np.save('nonFlare_class_full.npy', nonFlare_class)\n",
    "\n",
    "else:\n",
    "    nonFlare_data = np.load('flare_cycle24+/nonFlare_data_full.npy')\n",
    "    nonFlare_class = np.load('flare_cycle24+/nonFlare_class_full.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the number of events associated with the negative class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(nonFlare_data), \"noFlare events in the negative class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio: negative class to positive class\n",
    "ratio = len(nonFlare_data)/len(Flare_data)\n",
    "print(\"Ratio of negative class to positive class:\", ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "def normalize_together(positive_class, negative_class):\n",
    "    positive_class = np.array(positive_class)\n",
    "    negative_class = np.array(negative_class)\n",
    "    both_classes = np.concatenate((positive_class, negative_class), axis=0)\n",
    "    median_feature, std_feature = [], []\n",
    "    for j in range(both_classes.shape[1]):\n",
    "        standard_deviation_of_this_feature = np.std(both_classes[:, j])\n",
    "        median_of_this_feature = np.median(both_classes[:, j])\n",
    "        median_feature.append(median_of_this_feature)\n",
    "        std_feature.append(standard_deviation_of_this_feature)\n",
    "        for i in range(positive_class.shape[0]):\n",
    "            positive_class[i, j] = (\n",
    "                positive_class[i, j] - median_of_this_feature) / (standard_deviation_of_this_feature)\n",
    "        for i in range(negative_class.shape[0]):\n",
    "            negative_class[i, j] = (\n",
    "                negative_class[i, j] - median_of_this_feature) / (standard_deviation_of_this_feature)\n",
    "    return positive_class, negative_class, median_feature, std_feature\n",
    "\n",
    "\n",
    "Flare_data, nonFlare_data, median_feature, std_feature = normalize_together(Flare_data, nonFlare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the data\n",
    "\n",
    "N_features = Flare_data.shape[1]\n",
    "Nfl = Flare_data.shape[0]\n",
    "Nnofl = nonFlare_data.shape[0]\n",
    "yfl = np.ones(Nfl)\n",
    "ynofl = np.zeros(Nnofl)\n",
    "\n",
    "xdata = np.concatenate((Flare_data, nonFlare_data), axis=0)\n",
    "ydata = np.concatenate((yfl, ynofl), axis=0)\n",
    "mdata = np.concatenate((Flare_class, nonFlare_class), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Feature selection (filter methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step before starting to use any predictive method is to study the information present in the data. To do this, we are going to study the distribution of the characteristics of the active regions that produced a CME and those that did not. We will see the distribution of a feature for the active regions that produced a CME (green) and for the active regions that did not produce a CME (red). You can change the value of `i` in the code block below to see that some features are not at all useful since there is hardly any difference in the distributions for the positive and negative class. Therefore, we could discard those features from our sample before starting the modeling.\n",
    "\n",
    "This exercise is commonly referred to as Feature filtering and is based on the statistical properties of the features, such as correlation, variance, etc. These methods are fast and scalable, but do not consider the interactions between features or the predictive power of the subset of features. Other feature selection methods will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharps = ['Total unsigned flux', 'Mean gradient of total field',\n",
    "          'Mean current helicity (Bz contribution)', 'Mean photospheric magnetic free energy',\n",
    "          'Fraction of Area with Shear > 45 deg', 'Total unsigned current helicity',\n",
    "          'Mean gradient of horizontal field', 'Mean characteristic twist parameter, alpha',\n",
    "          'Mean angle of field from radial', 'Mean gradient of vertical field',\n",
    "          'Mean vertical current density', 'Total unsigned vertical current',\n",
    "          'Sum of the modulus of the net current per polarity',\n",
    "          'Total photospheric magnetic free energy density', 'Mean shear angle',\n",
    "          'Area of strong field pixels in the active region', 'Sum of flux near polarity inversion line',\n",
    "          'Absolute value of the net current helicity']\n",
    "\n",
    "sharp_mini = ['USFLUX', 'MEANGBT', 'MEANJZH', 'MEANPOT', 'SHRGT45', 'TOTUSJH', 'MEANGBH', 'MEANALP', 'MEANGAM', 'MEANGBZ', 'MEANJZD', 'TOTUSJZ', 'SAVNCPP', 'TOTPOT', 'MEANSHR', 'AREA_ACR', 'R_VALUE', 'ABSNJZH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random quantity is inserted to check the behaviour of the model\n",
    "# q = 8\n",
    "# sharps[q] = 'Random quantity'\n",
    "# Flare_data[:,q] = np.random.normal(0, 1, Flare_data.shape[0])\n",
    "# nonFlare_data[:,q] = np.random.normal(0, 1, nonFlare_data.shape[0])\n",
    "# print('The quantity', sharps[q], 'is a fake quantity.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# For the positive class (green)\n",
    "mu_fl = np.mean(Flare_data[:, i])\n",
    "sigma_fl = np.std(Flare_data[:, i])\n",
    "num_bins = 15\n",
    "n_fl, bins_fl, patches_fl = plt.hist(Flare_data[:, i], num_bins, facecolor='C2', alpha=0.5,density=True)\n",
    "y_fl = scipy.stats.norm.pdf(bins_fl, mu_fl, sigma_fl)\n",
    "plt.plot(bins_fl, y_fl, 'C2--', label='positive class (flare)')\n",
    "\n",
    "# For the negative class (red)\n",
    "mu_nofl = np.mean(nonFlare_data[:, i])\n",
    "sigma_nofl = np.std(nonFlare_data[:, i])\n",
    "n_nofl, bins_nofl, patches_nofl = plt.hist(nonFlare_data[:, i], num_bins, facecolor='C3', alpha=0.5,density=True)\n",
    "y_nofl = scipy.stats.norm.pdf(bins_nofl, mu_nofl, sigma_nofl)\n",
    "plt.plot(bins_nofl, y_nofl, 'C3--', label='negative class (no flare)')\n",
    "\n",
    "plt.xlabel('Normalized '+sharps[i], fontsize=12)\n",
    "plt.ylabel('Number (normalized)', fontsize=12)\n",
    "plt.minorticks_on()\n",
    "plt.locator_params(axis='y', nbins=6)\n",
    "legend = plt.legend(loc='upper right', fontsize=12, framealpha=0.0, title='')\n",
    "# plt.savefig('figures/feature_'+str(i)+'.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the Univariate F-score for feature selection. It is a very simple method: the F-score measures the distance between the two distributions for a given feature (inter-class distance), divided by the sum of the variances for this feature (intra-class distance). We can use the `sklearn.feature_selection` module to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the feature selection method\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# k is the number of features\n",
    "selector = SelectKBest(f_classif, k=N_features)\n",
    "selector.fit(xdata, ydata)\n",
    "scores = selector.scores_\n",
    "\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "order = np.argsort(selector.scores_)\n",
    "orderedsharps = [sharps[i] for i in order]\n",
    "orderedscores = [selector.scores_[i]/np.max(selector.scores_) for i in order]\n",
    "y_pos2 = np.arange(N_features)\n",
    "bars = plt.barh(y_pos2, orderedscores, color='C3', alpha=0.8, height=0.8)\n",
    "plt.yticks(y_pos2, orderedsharps, fontsize=12)\n",
    "plt.xlabel('Normalized Fisher Score', fontsize=12)\n",
    "ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False) \n",
    "ax.xaxis.set_visible(False)\n",
    "\n",
    "def custom_fmt(x):\n",
    "    return '<0.01' if x < 0.01 else '%.2f' % x\n",
    "\n",
    "ax.bar_label(bars, padding=+5, color='C3', \n",
    "             fontsize=12, label_type='edge', fmt=custom_fmt,\n",
    "             fontweight='bold')\n",
    "# Add title:\n",
    "plt.title('Normalized Fisher Score for each feature')\n",
    "# plt.savefig('figures/Fisher_score.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some moment I could consider to filter the features with a Fisher score below a certain threshold and only use the most important features. However, for now, I will use all the features because some of them could be important when combined with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the support vector machine on the data. Since we have 18 features, the SVM constructs an 18-dimensional feature space. In this feature space, the decision boundary separating the positive and negative examples may be non-linear. As such, the algorithm then enlarges this 18-dimensional feature space (using the function indicated by the `kernel` parameter in the [`svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) function) into a higher-dimensional feature space wherein it is possible to linearly separate the positive and negatives classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1e0\n",
    "gamma = 1e0\n",
    "clf = svm.SVC(C=C, gamma=gamma, kernel='rbf', class_weight='balanced',\n",
    "              cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratified k-folds cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of different ways to evaluate the performance of a classifier. We're going to choose a metric called the True Skill Score, or the TSS, which we can calculate from four quantities: true positives, true negatives, false positives, and false negatives. We prefer the TSS to all the other metrics as it is insensitive to the class imbalance ratio and thus best for comparison to other studies. The TSS is symmetrically distributed about 0: i.e., it goes from [-1, 1] where 0 represents no skill and a negative value represents a perverse prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_table(pred, labels):\n",
    "    \"\"\"\n",
    "    computes the number of TP, TN, FP, FN events given the arrays with predictions and true labels\n",
    "    and returns the true skill score\n",
    "\n",
    "    Args:\n",
    "    pred: np array with predictions (1 for flare, 0 for nonflare)\n",
    "    labels: np array with true labels (1 for flare, 0 for nonflare)\n",
    "\n",
    "    Returns: true negative, false positive, true positive, false negative\n",
    "    \"\"\"\n",
    "    Nobs = len(pred)\n",
    "    TN = 0.\n",
    "    TP = 0.\n",
    "    FP = 0.\n",
    "    FN = 0.\n",
    "    for i in range(Nobs):\n",
    "        if (pred[i] == 0 and labels[i] == 0):\n",
    "            TN += 1\n",
    "        elif (pred[i] == 1 and labels[i] == 0):\n",
    "            FP += 1\n",
    "        elif (pred[i] == 1 and labels[i] == 1):\n",
    "            TP += 1\n",
    "        elif (pred[i] == 0 and labels[i] == 1):\n",
    "            FN += 1\n",
    "        else:\n",
    "            print(\"Error! Observation could not be classified.\")\n",
    "    return TN, FP, TP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the SVM on our data and cross-validate our results. In our case, the positive sample size is quite small (both objectively and compared to the negative sample size). Therefore, we use a stratified k-folds cross-validation method, which makes k partitions of the data set and uses k-1 folds for training the SVM and 1 fold for testing the trained SVM. The stratification preserves the ratio of positive to negative examples per fold. Then we can permute over the partitions such that each partition eventually makes its way into the testing set. For each individual testing set, we can calculate a skill score. Then we can average the skill scores over the total number of testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consistency, we initialize again the classifier\n",
    "C = 1e0\n",
    "gamma = 1e0\n",
    "clf = svm.SVC(C=C, gamma=gamma, kernel='rbf', class_weight='balanced',\n",
    "              cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n",
    "\n",
    "\n",
    "k = 4 # 25% of the data is used for testing\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# skf = RepeatedStratifiedKFold(n_splits=k, n_repeats=10)\n",
    "these_TSS_for_this_k = []\n",
    "confusion_matrix_k = []\n",
    "for train_index, test_index in skf.split(xdata, ydata):\n",
    "    # xtrain are the examples in the training set\n",
    "    xtrain = xdata[train_index]\n",
    "    # ytrain are the labels in the training set\n",
    "    ytrain = ydata[train_index]\n",
    "    # xtest are the examples in the testing set\n",
    "    xtest = xdata[test_index]\n",
    "    # ytest are the labels in the testing set\n",
    "    ytest = ydata[test_index]    \n",
    "    # metadata useful for interpreting with LIME\n",
    "    mtrain = mdata[train_index]\n",
    "    # metadata useful for interpreting with LIME\n",
    "    mtest = mdata[test_index]\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "    if (((TP+FN) == 0.0) or (FP+TN) == 0.0):\n",
    "        these_TSS_for_this_k.append(np.nan)\n",
    "        continue\n",
    "    else:\n",
    "        these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "        confusion_matrix_k.append([TN, FP, TP, FN])\n",
    "    # break\n",
    "confusion_matrix_ = np.mean(confusion_matrix_k, axis=0)\n",
    "TSS_k = np.array(these_TSS_for_this_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, TP, FN = confusion_matrix_\n",
    "confusion_matrix = np.array([TP, FN, FP, TN]).reshape(2, 2)\n",
    "print('Our classifier has a True Skill Score of', TP/(TP+FN) - FP/(FP+TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=['Flare','No Flare'])\n",
    "display.plot(cmap='Blues')\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "# Now normalize the confusion matrix\n",
    "display = ConfusionMatrixDisplay(confusion_matrix= (confusion_matrix.T / np.sum(confusion_matrix,axis=1)).T, display_labels=['Flare','No Flare'])\n",
    "display.plot(cmap='Blues')\n",
    "plt.title('Normalized confusion matrix')\n",
    "for im in plt.gca().get_images():                   # set clim manually within the image\n",
    "    im.set_clim(vmin=0,vmax=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will tune the hyperparameters of the SVM model. We will use different values for the hyperparameters and calculate the TSS score for each combination. We will then choose the hyperparameters that give the best TSS score. However we can find cases where the method is so flexible that it can fit every data point in the data. This is called overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSS\n",
    "TSS_tun_list = []\n",
    "\n",
    "# Suffling xdata and ydata:\n",
    "idx = np.arange(len(xdata))\n",
    "np.random.shuffle(idx)\n",
    "xdata = xdata[idx]\n",
    "ydata = ydata[idx]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "param_grid = {'C': 10**np.arange(-1, 2, 1.0),\n",
    "              'gamma': 10**np.arange(-1, 2, 1.0)}\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "skf = RepeatedStratifiedKFold(n_splits=4, n_repeats=5)\n",
    "for CC in param_grid['C']:\n",
    "    for GG in param_grid['gamma']:\n",
    "\n",
    "        clf = svm.SVC(C=CC, gamma=GG, kernel='rbf', class_weight='balanced',\n",
    "                    cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n",
    "        \n",
    "        these_TSS_for_this_k = []\n",
    "        for train_index, test_index in skf.split(xdata, ydata):\n",
    "            xtrainq = xdata[train_index]\n",
    "            ytrainq = ydata[train_index]\n",
    "            xtestq = xdata[test_index]\n",
    "            ytestq = ydata[test_index]\n",
    "            \n",
    "\n",
    "            clf.fit(xtrainq, ytrainq)\n",
    "            \n",
    "            TN, FP, TP, FN = confusion_table(clf.predict(xtestq), ytestq)\n",
    "            if (((TP+FN) == 0.0) or (FP+TN) == 0.0) or (TP == 0.0) or (FP == 0.0):\n",
    "                these_TSS_for_this_k.append(np.nan)\n",
    "                continue\n",
    "            else:\n",
    "                these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "                # print(TP,FP,FN)\n",
    "                # these_TSS_for_this_k.append(TP/(TP+FP))\n",
    "        TSS_k = np.array(these_TSS_for_this_k)\n",
    "        TSS_tun_list.append(np.mean(TSS_k))\n",
    "\n",
    "TSS_tuned = np.array(TSS_tun_list).reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "# replace nans by 0.0\n",
    "TSS_tuned = np.nan_to_num(TSS_tuned)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=0.15, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(TSS_tuned, interpolation='nearest', cmap='CMRmap',vmin=None, vmax=None)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.xticks(np.arange(len(param_grid['gamma'])), ['%.1e' % i for i in param_grid['gamma']])\n",
    "plt.yticks(np.arange(len(param_grid['C'])), ['%.1e' % i for i in param_grid['C']])\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('TSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_2d_range = [1e-1, 1, 10]\n",
    "gamma_2d_range = [1e-1, 1, 10]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf_ = svm.SVC(C=C, gamma=gamma, kernel='rbf', class_weight='balanced',\n",
    "                cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n",
    "        clf_.fit(xdata[:, :2], ydata)\n",
    "        classifiers.append((C, gamma, clf_))\n",
    "        \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "xx, yy = np.meshgrid(np.linspace(xdata[:, 0].min()*1.5, xdata[:, 0].max(), 100), np.linspace(xdata[:, 1].min()*1.5, xdata[:, 1].max(), 100))\n",
    "for k, (C, gamma, clf_) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    Z = clf_.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "    plt.title(r\"$\\gamma$=$10^{%d}$, C=$10^{%d}$\" % (np.log10(gamma), np.log10(C)), size=\"medium\")\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(xdata[:, 0], xdata[:, 1], c=ydata, cmap=plt.cm.RdBu_r, s=1)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.axis(\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Feature selection (wrapper methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1e0\n",
    "gamma = 1e0\n",
    "clf_q = svm.SVC(C=C, gamma=gamma, kernel='rbf', class_weight='balanced',\n",
    "              cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n",
    "\n",
    "\n",
    "# Sequential feature selection: adding features one by one\n",
    "\n",
    "TSS_for_this_combination = []\n",
    "# features_to_use is the index saved in best_combination and the index of the feature we are testing\n",
    "for feature_to_test in range(N_features):\n",
    "\n",
    "    # skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "    skf = RepeatedStratifiedKFold(n_splits=4, n_repeats=2)\n",
    "    these_TSS_for_this_k = []\n",
    "    for train_index, test_index in skf.split(xdata, ydata):\n",
    "        xtrain = xdata[train_index][:, feature_to_test:feature_to_test+1]\n",
    "        ytrain = ydata[train_index]\n",
    "        xtest = xdata[test_index][:, feature_to_test:feature_to_test+1]\n",
    "        mtest = mdata[test_index]\n",
    "        ytest = ydata[test_index]\n",
    "        clf_q.fit(xtrain, ytrain)\n",
    "        TN, FP, TP, FN = confusion_table(clf_q.predict(xtest), ytest)\n",
    "        these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "    TSS = np.mean(these_TSS_for_this_k)\n",
    "    \n",
    "    TSS_for_this_combination.append(TSS)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "norder = np.argsort(TSS_for_this_combination)\n",
    "norderedsharps = [sharps[i] for i in norder]\n",
    "nscores = [selector.scores_[i] for i in norder]\n",
    "y_pos2 = np.arange(18)\n",
    "bars = plt.barh(y_pos2, nscores/np.max(scores), color='C3', alpha=0.8, height=0.8)\n",
    "plt.yticks(y_pos2, norderedsharps, fontsize=12)\n",
    "plt.xlabel('Normalized Fisher Score', fontsize=12)\n",
    "ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False) \n",
    "ax.xaxis.set_visible(False)\n",
    "\n",
    "plt.title('Normalized Fisher Score, ordered by single TSS score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sequential feature selection: adding features one by one\n",
    "\n",
    "total_features = N_features\n",
    "\n",
    "best_combination = []\n",
    "TSS_for_best_combination = []\n",
    "for combination in range(1, total_features+1):\n",
    "    TSS_for_this_combination = []\n",
    "    # features_to_use is the index saved in best_combination and the index of the feature we are testing\n",
    "    for feature_to_test in range(total_features):\n",
    "        if feature_to_test in best_combination:\n",
    "            TSS_for_this_combination.append(0.0)\n",
    "            continue\n",
    "        features_to_use = best_combination.copy()\n",
    "        features_to_use.append(feature_to_test)\n",
    "\n",
    "        # skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "        skf = RepeatedStratifiedKFold(n_splits=4, n_repeats=2)\n",
    "        these_TSS_for_this_k = []\n",
    "        for train_index, test_index in skf.split(xdata, ydata):\n",
    "            xtrain = xdata[train_index][:, features_to_use]\n",
    "            ytrain = ydata[train_index]\n",
    "            xtest = xdata[test_index][:, features_to_use]        \n",
    "            # mtest = mdata[test_index]\n",
    "            ytest = ydata[test_index]\n",
    "            clf.fit(xtrain, ytrain)\n",
    "            TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "            these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "        TSS = np.mean(these_TSS_for_this_k)\n",
    "        \n",
    "        TSS_for_this_combination.append(TSS)\n",
    "    # print('All TSS for this combination:', TSS_for_this_combination)\n",
    "    \n",
    "    best_combination.append(np.argmax(TSS_for_this_combination))\n",
    "    TSS_for_best_combination.append(np.max(TSS_for_this_combination))\n",
    "    print(\"The best combination so far is\", best_combination, \"with a TSS of\", \"{:.2f}\".format(np.max(TSS_for_this_combination)), \n",
    "          \"where addition had a score of\", \"{:.2f}\".format(selector.scores_[np.argmax(TSS_for_this_combination)]/np.max(selector.scores_)))\n",
    "\n",
    "    # break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important trend we can already see is that as we start adding too many features, the TSS starts to decrease.\n",
    "plt.plot(np.arange(1, total_features+1), TSS_for_best_combination, 'o-', color='C4')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('TSS')\n",
    "plt.title('TSS as a function of the number of features')\n",
    "plt.xticks(np.arange(1, total_features+1))\n",
    "plt.ylim(ymax=1.0, ymin=0)\n",
    "plt.locator_params(axis='x', nbins=9)\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='black', alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can observe that using only the top 5 features, we can achieve a TSS as good as using all 18 features. This is a significant reduction in the number of features we need to consider. This is a very important result, as it means that we can reduce the computational cost of our model by a factor of 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Agnostic \"local\" Explanations (Shapley values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine-learning is a powerful technique that can help us predict certain events. However, our goal is not only to predict them, but also to quantitatively understand which signatures indicate the imminent triggering of the event. To do this, we can use different tools (like the SHAP library) to explain the model's predictions. SHAP values represent the contribution of each feature to the prediction as an additive feature attribution, so one can understand the importance of each feature in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical train-test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.25, random_state=42)\n",
    "clf.fit(xtrain, ytrain)\n",
    "TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "print([TN, FP, TP, FN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "# KernelExplainer is the model that uses a special weighted linear regression \n",
    "# to compute the importance of each feature.\n",
    "explainer = shap.KernelExplainer(clf.predict_proba, shap.kmeans(xdata, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, xtest.shape[0])\n",
    "shap_values = explainer.shap_values(xtest[i, :])\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[:, 1], xtest[i, :], feature_names=sharps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the force plot shows the contribution of each feature to the prediction. The color represents the value of the feature (red for high values, blue for low values). The features are ordered by importance, with the most important features at the top. The larger the arrow, the more the feature contributes to the prediction. The base value is the average prediction over the training set. The output value is the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the event in the metadata:\n",
    "index_event = np.where(np.all(xdata == xtest[i, :], axis=1))[0][0]\n",
    "print(\"The AR number\", mdata[index_event][1], \"had a flare of class\", mdata[index_event][2], \"at\", mdata[index_event][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the predictor to analyze new observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_index = 10\n",
    "\n",
    "# Load the selected features and metadata\n",
    "selected_metadata = np.load('cme_cycle24+/student_target_metadata.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the t_rec is not the time of the event but the time of the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rec = np.array([selected_metadata[student_index][-1]])\n",
    "listofactiveregions = np.array([selected_metadata[student_index][1]])\n",
    "listofgoesclasses = np.array([selected_metadata[student_index][2]])\n",
    "\n",
    "# Download the data from JSOC\n",
    "my_features, my_flareclass = get_the_jsoc_data(1, t_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features and run the classifier:\n",
    "my_features = (np.array(my_features) - median_feature)/std_feature\n",
    "print(\"Normalized features:\", my_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model prediction will colapse the output to the 0 or 1 class\n",
    "my_prediction = clf.predict(my_features)\n",
    "print('This AR belongs to the class:', my_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can use probabilities to get a more detailed prediction\n",
    "print(\"My model predicts a flare with a probability of\", clf.predict_proba(my_features)[0][1])\n",
    "print(\"and the metadata says that there was a flare of class\", my_flareclass[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(my_features[0,:])\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[:, 1], my_features[0,:], feature_names=sharps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
