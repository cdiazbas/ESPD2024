{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this prediction problem, we'll then ascribe each active region to one of two classes:\n",
    "\n",
    "1. The positive class contains flaring active regions that did produce a CME. \n",
    "\n",
    "2. The negative class contains flaring active regions that did not produce a CME. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our CME predictor we need to gather our training dataset: \n",
    "\n",
    "1. CME data from SOHO/LASCO and STEREO/SECCHI coronographs, which can be accessed from the `Database Of Notifications, Knowledge, Information` [DONKI database](http://kauai.ccmc.gsfc.nasa.gov/DONKI/). This tells us if an active region has produced a CME or not.\n",
    "\n",
    "2. Flare data from the GOES flare catalog at NOAA, which can be accessed with the `sunkit_instruments` library, a SunPy affiliated package for solar instrument-specific tools. This tells us if an active region produced a flare or not.\n",
    "\n",
    "3. Active region data from the Solar Dynamics Observatory's `Helioseismic and Magnetic Imager instrument` (HMI), which can be accessed from the [JSOC database](http://jsoc.stanford.edu/) via a JSON API. This gives us the features characterizing each active region `SHARP` – Space-Weather HMI Active Region Patches [Bobra et al., 2014](http://link.springer.com/article/10.1007%2Fs11207-014-0529-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering data for the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "from datetime import datetime as dt_obj\n",
    "from datetime import timedelta\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Sunpy\n",
    "try:\n",
    "    import sunpy\n",
    "    from sunpy.time import TimeRange\n",
    "except ImportError:\n",
    "    %pip install sunpy\n",
    "    from sunpy.time import TimeRange\n",
    "\n",
    "# Lime library\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "except ImportError:\n",
    "    %pip install lime\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "    \n",
    "# Sunkit-instruments\n",
    "try:\n",
    "    import sunkit_instruments\n",
    "except ImportError:\n",
    "    %pip install sunkit_instruments\n",
    "    import sunkit_instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first query the [DONKI database](http://kauai.ccmc.gsfc.nasa.gov/DONKI/) to get the data associated with the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the data\n",
    "t_start = \"2010-05-01\" # Using solar cycle 24\n",
    "t_end = \"2011-05-01\"  # Date for example, but using until 2024 for the full dataset (takes ~30min)\n",
    "\n",
    "\n",
    "\n",
    "baseurl = \"https://kauai.ccmc.gsfc.nasa.gov/DONKI/WS/get/FLR?\"\n",
    "\n",
    "url = baseurl+\"startDate=\"+t_start+\"&endDate=\"+t_end\n",
    "print('[INFO] requesting data from', url)\n",
    "\n",
    "flag_for_fulldataset = False\n",
    "print('[INFO] flag_for_fulldataset:', flag_for_fulldataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go to one of those links we can see what information is available. Try https://kauai.ccmc.gsfc.nasa.gov/DONKI/view/CME/29563/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.- we will focus only on the strongest flares: M and X class\n",
    "# 2.- we will drop the rows that aren't linked to CME events\n",
    "events_list = df.loc[df['classType'].str.contains(\"M|X\") & df['linkedEvents'].apply(lambda x: any('CME' in i['activityID'] for i in x) if x is not None else False)] # & df['activeRegionNum'].apply(lambda x: np.nan_to_num(x) > 0)]\n",
    "\n",
    "# Drop the most of columns for a better visualization after the filtering\n",
    "events_list = events_list.drop(columns=['flrID','linkedEvents','instruments','beginTime','endTime','link'])\n",
    "events_list = events_list.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len_events = events_list.shape[0]\n",
    "print(\"There are\", len_events, \"events in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the `peakTime` column in the `events_list` dataframe from a string into a datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tai_string(tstr):\n",
    "    year = int(tstr[:4])\n",
    "    month = int(tstr[5:7])\n",
    "    day = int(tstr[8:10])\n",
    "    hour = int(tstr[11:13])\n",
    "    minute = int(tstr[14:16])\n",
    "    return dt_obj(year, month, day, hour, minute)\n",
    "\n",
    "\n",
    "for i in range(events_list.shape[0]):\n",
    "    events_list.loc[i, 'peakTime'] = parse_tai_string(events_list.loc[i, 'peakTime'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to improve the data quality. We need to check 3 cases:\n",
    "1. The DONKI database does not have the active region number.\n",
    "2. The DONKI database has the active region number but it is not the correct one.\n",
    "3. The DONKI database has the peak time of the flare but it is not the correct one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Case 1: In this case, the CME and flare exist but NOAA active region number does not exist in the DONKI database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sunkit_instruments.goes_xrs\n",
    "\n",
    "# We will use the GOES flare database to find the missing NOAA active region numbers\n",
    "\n",
    "number_of_donki_mistakes = 0  # count the number of DONKI mistakes\n",
    "\n",
    "# Create a list to store the indices of the rows that will be dropped\n",
    "event_list_drops = []\n",
    "for i in range(events_list.shape[0]):\n",
    "    \n",
    "    # print(events_list.loc[i]['activeRegionNum'])\n",
    "    # If the active region number is NaN, we will find the missing NOAA active region number\n",
    "    if (np.isnan(events_list.loc[i]['activeRegionNum'])):\n",
    "        time = events_list.loc[i]['peakTime']\n",
    "        time_range = TimeRange(time, time)\n",
    "        # A string specifying a minimum GOES class for inclusion in the list, e.g., “M1”, “X2”.\n",
    "        listofresults =  sunkit_instruments.goes_xrs.get_goes_event_list(time_range, 'M1')\n",
    "        \n",
    "        if len(listofresults) < 0.5:\n",
    "            # When sunkit fails to find any event that day\n",
    "            print(time,events_list.loc[i]['classType'], \"has no match in the GOES flare database ; dropping row.\")\n",
    "            event_list_drops.append(i)\n",
    "            number_of_donki_mistakes += 1\n",
    "            continue\n",
    "        \n",
    "        if (listofresults[0]['noaa_active_region'] == 0):\n",
    "            print(time,events_list.loc[i]['activeRegionNum'], events_list.loc[i]\n",
    "                  ['classType'], \"has no match in the GOES flare database ; dropping row.\")\n",
    "            event_list_drops.append(i)\n",
    "            number_of_donki_mistakes += 1\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Missing NOAA number:\", events_list.loc[i]['activeRegionNum'], events_list.loc[i]['classType'],\n",
    "                  events_list['peakTime'].iloc[i], \"should be\", listofresults[0]['noaa_active_region'], \"; changing now.\")\n",
    "            events_list.loc[i, 'activeRegionNum'] = listofresults[0]['noaa_active_region']\n",
    "            number_of_donki_mistakes += 1\n",
    "\n",
    "# Drop the rows for which there is no active region number in both the DONKI and GOES flare databases\n",
    "events_list = events_list.drop(event_list_drops)\n",
    "events_list = events_list.reset_index(drop=True)\n",
    "print('There are', number_of_donki_mistakes, 'DONKI mistakes so far.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we grab all the data from the GOES database in preparation for checking Cases 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab all the data from the GOES database\n",
    "time_range = TimeRange(t_start, t_end)\n",
    "listofresults = sunkit_instruments.goes_xrs.get_goes_event_list(time_range, 'M1')\n",
    "print('Grabbed all the GOES data; there are', len(listofresults), 'events.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Case 2: In this case, the NOAA active region number is wrong in the DONKI database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Case 2: NOAA active region number is wrong in DONKI database\n",
    "\n",
    "# collect all the peak flares times in the NOAA database\n",
    "peak_times_noaa = [item[\"peak_time\"] for item in listofresults]\n",
    "\n",
    "for i in range(events_list.shape[0]):\n",
    "    # check if a particular DONKI flare peak time is also in the NOAA database\n",
    "    peak_time_donki = events_list['peakTime'].iloc[i]\n",
    "    \n",
    "    # If the peak time is in the NOAA database, we can compare the active region numbers!\n",
    "    if peak_time_donki in peak_times_noaa:\n",
    "        index = peak_times_noaa.index(peak_time_donki)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # ignore NOAA active region numbers equal to zero\n",
    "    if (listofresults[index]['noaa_active_region'] == 0):\n",
    "        continue\n",
    "    \n",
    "    # if yes, check if the DONKI and NOAA active region numbers match up for this peak time\n",
    "    # if they don't, flag this peak time and replace the DONKI number with the NOAA number\n",
    "    if (listofresults[index]['noaa_active_region'] != int(events_list['activeRegionNum'].iloc[i])):\n",
    "        print('Messed up NOAA number:', int(events_list['activeRegionNum'].iloc[i]), events_list['classType'].iloc[i],\n",
    "              events_list['peakTime'].iloc[i], \"should be\", listofresults[index]['noaa_active_region'], \"; changing now.\")\n",
    "        events_list.loc[i, 'activeRegionNum'] = listofresults[index]['noaa_active_region']\n",
    "        number_of_donki_mistakes += 1\n",
    "print('There are', number_of_donki_mistakes, 'DONKI mistakes so far.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Case 3: In this case, the flare peak time is wrong in the DONKI database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3: The flare peak time is wrong in the DONKI database.\n",
    "\n",
    "# create an empty array to hold row numbers to drop at the end\n",
    "event_list_drops = []\n",
    "\n",
    "active_region_numbers_noaa = [item[\"noaa_active_region\"]\n",
    "                              for item in listofresults]\n",
    "flare_classes_noaa = [item[\"goes_class\"] for item in listofresults]\n",
    "\n",
    "for i in range(events_list.shape[0]):\n",
    "    \n",
    "    # check if a particular DONKI flare peak time is also in the NOAA database\n",
    "    peak_time_donki = events_list['peakTime'].iloc[i]\n",
    "    \n",
    "    # If the peak time is not in the NOAA database, let's try to fix it\n",
    "    if not peak_time_donki in peak_times_noaa:\n",
    "        \n",
    "        active_region_number_donki = int(events_list.loc[i,'activeRegionNum'])\n",
    "        \n",
    "        flare_class_donki = events_list['classType'].iloc[i]\n",
    "        \n",
    "        # List of flares with the same name\n",
    "        flare_class_indices = [i for i, x in enumerate(\n",
    "            flare_classes_noaa) if x == flare_class_donki]\n",
    "        \n",
    "        # List of active regions with the same number\n",
    "        active_region_indices = [i for i, x in enumerate(\n",
    "            active_region_numbers_noaa) if x == active_region_number_donki]\n",
    "        \n",
    "        # Index of the common elements in the two lists\n",
    "        common_indices = list(\n",
    "            set(flare_class_indices).intersection(active_region_indices))\n",
    "        \n",
    "        # This common index is the index of the flare in the NOAA database\n",
    "        # which has the same active region number and flare class as the DONKI database\n",
    "        \n",
    "        if common_indices:\n",
    "            print(\"Messed up time:\", int(events_list['activeRegionNum'].iloc[i]), events_list.loc[i,'classType'],\n",
    "                  events_list.loc[i,'peakTime'], \"should be\", peak_times_noaa[common_indices[0]], \"; changing now.\")\n",
    "            events_list.loc[i, 'peakTime'] = peak_times_noaa[common_indices[0]]\n",
    "            number_of_donki_mistakes += 1\n",
    "            \n",
    "        if not common_indices:\n",
    "            print(\"DONKI flare peak time\",\n",
    "                  events_list.loc[i, 'peakTime'], \"has no match; dropping row.\")\n",
    "            event_list_drops.append(i)\n",
    "            number_of_donki_mistakes += 1\n",
    "\n",
    "# Drop the rows for which the NOAA active region number and flare class associated with\n",
    "# the messed-up flare peak time in the DONKI database has no match in the GOES flare database\n",
    "events_list = events_list.drop(event_list_drops)\n",
    "events_list = events_list.reset_index(drop=True)\n",
    "\n",
    "# Create a list of corrected flare peak times\n",
    "peak_times_donki = [events_list.loc[i, 'peakTime']\n",
    "                    for i in range(events_list.shape[0])]\n",
    "\n",
    "\n",
    "print('There are', number_of_donki_mistakes, 'DONKI mistakes so far.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final table of events that fall into the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "events_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's query the JSOC database to see if there are active region parameters at the time of the flare. First read the following file to map NOAA active region numbers to HARPNUMs (a HARP, or an HMI Active Region Patch, is the preferred numbering system for the HMI active regions as they appear in the magnetic field data before NOAA observes them in white light):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.read_csv(\n",
    "    'http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's determine at which time we'd like to predict CMEs. In general, many people try to predict a CME either 24 or 48 hours before it happens. We can report both in this study by setting a variable called `timedelayvariable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelayvariable = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll convert subtract `timedelayvariable` from the GOES Peak Time and re-format the datetime object into a string that JSOC can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rec = [(events_list['peakTime'].iloc[i] - timedelta(hours=timedelayvariable)\n",
    "          ).strftime('%Y.%m.%d_%H:%M_TAI') for i in range(events_list.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the SDO data from the JSOC database by executing the JSON queries. The SHARP parameters are calculated every 12 minutes during an AR lifetime. We are selecting data that satisfies several criteria: The data has to be [1] disambiguated with a version of the disambiguation module greater than 1.1, [2] taken while the orbital velocity of the spacecraft is less than 3500 m/s, [3] of a high quality, and [4] within 70 degrees of central meridian. If the data pass all these tests, they are put into one of two lists: one for the positive class (called CME_data) and one for the negative class (called no_CME_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_jsoc_data(event_count, t_rec):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_count: number of events \n",
    "                 int\n",
    "\n",
    "    t_rec:       list of times, one associated with each event in event_count\n",
    "                 list of strings in JSOC format ('%Y.%m.%d_%H:%M_TAI')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    catalog_data = []\n",
    "    classification = []\n",
    "\n",
    "    for i in range(event_count):\n",
    "\n",
    "        print(\"=====\", i,\"/\",event_count-1,\"=====\")\n",
    "        \n",
    "        # Check if the active region number is not zero:\n",
    "        if int(listofactiveregions[i]) == 0:\n",
    "            print('skip: NOAA Active Region number is zero')\n",
    "            continue\n",
    "        \n",
    "        # next match NOAA_ARS to HARPNUM\n",
    "        idx = answer[answer['NOAA_ARS'].str.contains(\n",
    "            str(int(listofactiveregions[i])))]\n",
    "\n",
    "        # if there's no HARPNUM, quit\n",
    "        if (idx.empty == True):\n",
    "            print('skip: there are no matching HARPNUMs for',\n",
    "                  str(int(listofactiveregions[i])))\n",
    "            continue\n",
    "\n",
    "        # construct jsoc_info queries and query jsoc database; we are querying for 25 keywords\n",
    "        url = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_720s[\"+str(\n",
    "            idx.HARPNUM.values[0])+\"][\"+t_rec[i]+\"][? (CODEVER7 !~ '1.1 ') and (abs(OBS_VR)< 3500) and (QUALITY<65536) ?]&op=rs_list&key=USFLUX,MEANGBT,MEANJZH,MEANPOT,SHRGT45,TOTUSJH,MEANGBH,MEANALP,MEANGAM,MEANGBZ,MEANJZD,TOTUSJZ,SAVNCPP,TOTPOT,MEANSHR,AREA_ACR,R_VALUE,ABSNJZH\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if there's no response at this time, quit\n",
    "        if response.status_code != 200:\n",
    "            print('skip: cannot successfully get an http response')\n",
    "            continue\n",
    "\n",
    "        # read the JSON output\n",
    "        data = response.json()\n",
    "\n",
    "        # if there are no data at this time, quit\n",
    "        if data['count'] == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        # check to see if the active region is too close to the limb\n",
    "        # we can compute the latitude of an active region in stonyhurst coordinates as follows:\n",
    "        # latitude_stonyhurst = CRVAL1 - CRLN_OBS\n",
    "        # for this we have to query the CEA series (but above we queried the other series as the CEA series does not have CODEVER5 in it)\n",
    "\n",
    "        url = \"http://jsoc.stanford.edu/cgi-bin/ajax/jsoc_info?ds=hmi.sharp_cea_720s[\"+str(\n",
    "            idx.HARPNUM.values[0])+\"][\"+t_rec[i]+\"][? (abs(OBS_VR)< 3500) and (QUALITY<65536) ?]&op=rs_list&key=CRVAL1,CRLN_OBS\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if there's no response at this time, quit\n",
    "        if response.status_code != 200:\n",
    "            print('skip: failed to find CEA JSOC data for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        # read the JSON output\n",
    "        latitude_information = response.json()\n",
    "\n",
    "        # if there are no data at this time, quit\n",
    "        if latitude_information['count'] == 0:\n",
    "            print('skip: there are no data for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        CRVAL1 = float(latitude_information['keywords'][0]['values'][0])\n",
    "        CRLN_OBS = float(latitude_information['keywords'][1]['values'][0])\n",
    "        if (np.absolute(CRVAL1 - CRLN_OBS) > 70.0):\n",
    "            print('skip: latitude is out of range for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        if ('MISSING' in str(data['keywords'])):\n",
    "            print('skip: there are some missing keywords for HARPNUM',\n",
    "                  idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "            continue\n",
    "\n",
    "        print('accept NOAA Active Region number', str(int(\n",
    "            listofactiveregions[i])), 'and HARPNUM', idx.HARPNUM.values[0], 'at time', t_rec[i])\n",
    "\n",
    "        individual_flare_data = []\n",
    "        for j in range(18):\n",
    "            individual_flare_data.append(\n",
    "                float(data['keywords'][j]['values'][0]))\n",
    "\n",
    "        catalog_data.append(list(individual_flare_data))\n",
    "\n",
    "        single_class_instance = [idx.HARPNUM.values[0], str(\n",
    "            int(listofactiveregions[i])), listofgoesclasses[i], t_rec[i]]\n",
    "        classification.append(single_class_instance)\n",
    "\n",
    "    return catalog_data, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data to be fed into the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofactiveregions = list(events_list['activeRegionNum'].values.flatten())\n",
    "listofgoesclasses = list(events_list['classType'].values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "CME_data, positive_class = get_the_jsoc_data(events_list.shape[0], t_rec)\n",
    "\n",
    "# save the data\n",
    "if flag_for_fulldataset:\n",
    "    np.save('CME_data_full.npy', CME_data)\n",
    "    np.save('positive_class_full.npy', positive_class)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the number of events associated with the positive class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(CME_data), \"CME events in the positive class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Gathering data for the negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather the examples for the negative class, we only need to:\n",
    "\n",
    "1. Query the GOES database for all the M- and X-class flares during our time of interest, and\n",
    "2. Select the ones that are not associated with a CME. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select peak times that belong to both classes\n",
    "all_peak_times = np.array([(listofresults[i]['peak_time'])\n",
    "                           for i in range(len(listofresults))])\n",
    "\n",
    "negative_class_possibilities = []\n",
    "counter_positive = 0\n",
    "counter_negative = 0\n",
    "for i in range(len(listofresults)):\n",
    "    this_peak_time = all_peak_times[i]\n",
    "    if (this_peak_time in peak_times_donki):\n",
    "        counter_positive += 1\n",
    "    else:\n",
    "        counter_negative += 1\n",
    "        this_instance = [listofresults[i]['noaa_active_region'],\n",
    "                         listofresults[i]['goes_class'], listofresults[i]['peak_time']]\n",
    "        negative_class_possibilities.append(this_instance)\n",
    "print(\"There are\", counter_positive, \"events in the positive class.\")\n",
    "print(\"There are\", counter_negative, \"events in the negative class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we compute times that are one day before the flare peak time and convert it into a string that JSOC can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rec = np.array([(negative_class_possibilities[i][2] - timedelta(hours=timedelayvariable)\n",
    "                   ).strftime('%Y.%m.%d_%H:%M_TAI') for i in range(len(negative_class_possibilities))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we query the JSOC database to see if these data are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofactiveregions = list(\n",
    "    negative_class_possibilities[i][0] for i in range(counter_negative))\n",
    "listofgoesclasses = list(\n",
    "    negative_class_possibilities[i][1] for i in range(counter_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the data and save the data\n",
    "if flag_for_fulldataset:\n",
    "    no_CME_data, negative_class = get_the_jsoc_data(counter_negative, t_rec)\n",
    "\n",
    "    np.save('no_CME_data_full.npy', no_CME_data)\n",
    "    np.save('negative_class_full.npy', negative_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the number of events associated with the negative class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the data and save the data\n",
    "if flag_for_fulldataset:\n",
    "    print(\"There are\", len(no_CME_data), \"no-CME events in the negative class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features within a data set may be powerful for distinguishing between the positive and negative class, whereas others may be redundant or irrelevant. To identify features in the former category, we use a univariate feature selection method, which is implemented in the feature selection module of the scikit-learn library, for feature scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the feature selection algorithm, we'll normalize each feature so that they lie within similar ranges. To do this, we subtract from every feature its median value and divide by its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "def normalize_together(positive_class, negative_class):\n",
    "    both_classes = np.concatenate((positive_class, negative_class), axis=0)\n",
    "    for j in range(both_classes.shape[1]):\n",
    "        standard_deviation_of_this_feature = np.std(both_classes[:, j])\n",
    "        median_of_this_feature = np.median(both_classes[:, j])\n",
    "        for i in range(positive_class.shape[0]):\n",
    "            positive_class[i, j] = (\n",
    "                positive_class[i, j] - median_of_this_feature) / (standard_deviation_of_this_feature)\n",
    "        for i in range(negative_class.shape[0]):\n",
    "            negative_class[i, j] = (\n",
    "                negative_class[i, j] - median_of_this_feature) / (standard_deviation_of_this_feature)\n",
    "    return positive_class, negative_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepare_student_data = False\n",
    "if prepare_student_data:\n",
    "\n",
    "    CME_data = np.load('cme_cycle24+/CME_data_full.npy')\n",
    "    positive_class = np.load('cme_cycle24+/positive_class_full.npy')\n",
    "\n",
    "    no_CME_data = np.load('cme_cycle24+/no_CME_data_full.npy')\n",
    "    negative_class = np.load('cme_cycle24+/negative_class_full.npy')\n",
    "    \n",
    "    no_CME_data, CME_data = normalize_together(no_CME_data, CME_data)\n",
    "        \n",
    "    # Note: Extract 50 events and their corresponding features for testing later:\n",
    "    index_positive = np.random.choice(range(len(positive_class)), 50, replace=False)\n",
    "    index_negative = np.random.choice(range(len(negative_class)), 0, replace=False)\n",
    "    \n",
    "    # Extract the features and metadata for the selected events, and delete them from the full dataset\n",
    "    selected_features = np.concatenate((CME_data[index_positive], no_CME_data[index_negative]), axis=0)\n",
    "    selected_metadata = np.concatenate((positive_class[index_positive], negative_class[index_negative]), axis=0)\n",
    "    \n",
    "    # Shuffle the selected features and metadata to ensure they are mixed\n",
    "    # shuffle_index = np.random.permutation(len(selected_features))\n",
    "    # selected_features = selected_features[shuffle_index]\n",
    "    # selected_metadata = selected_metadata[shuffle_index]\n",
    "    \n",
    "    # Save the selected features and metadata\n",
    "    np.save('cme_cycle24+/student_target_features.npy', selected_features)\n",
    "    np.save('cme_cycle24+/student_target_metadata.npy', selected_metadata)\n",
    "    \n",
    "    # Remove the selected features from the full dataset\n",
    "    CME_data = np.delete(CME_data, index_positive, axis=0)\n",
    "    no_CME_data = np.delete(no_CME_data, index_negative, axis=0)\n",
    "    positive_class = np.delete(positive_class, index_positive, axis=0)\n",
    "    negative_class = np.delete(negative_class, index_negative, axis=0)\n",
    "    \n",
    "    # Save the new datasets\n",
    "    np.save('cme_cycle24+/CME_data_.npy', CME_data)\n",
    "    np.save('cme_cycle24+/positive_class_.npy', positive_class)\n",
    "    np.save('cme_cycle24+/no_CME_data_.npy', no_CME_data)\n",
    "    np.save('cme_cycle24+/negative_class_.npy', negative_class)\n",
    "\n",
    "    \n",
    "else:\n",
    "    # Load the selected features and metadata\n",
    "    selected_features = np.load('cme_cycle24+/student_target_features.npy')\n",
    "    selected_metadata = np.load('cme_cycle24+/student_target_metadata.npy')\n",
    "    \n",
    "    # Load the training dataset\n",
    "    CME_data = np.load('cme_cycle24+/CME_data_.npy')\n",
    "    positive_class = np.load('cme_cycle24+/positive_class_.npy')\n",
    "    no_CME_data = np.load('cme_cycle24+/no_CME_data_.npy')\n",
    "    negative_class = np.load('cme_cycle24+/negative_class_.npy')\n",
    "    \n",
    "    # # Load the training dataset\n",
    "    # CME_data = np.load('cme_cycle24+/CME_data_full.npy')\n",
    "    # positive_class = np.load('cme_cycle24+/positive_class_full.npy')\n",
    "    # no_CME_data = np.load('cme_cycle24+/no_CME_data_full.npy')\n",
    "    # negative_class = np.load('cme_cycle24+/negative_class_full.npy')\n",
    "    \n",
    "    # no_CME_data, CME_data = normalize_together(no_CME_data, CME_data)\n",
    "\n",
    "    print(\"There are\", no_CME_data.shape[0], \"flares with no associated CMEs.\")\n",
    "    print(\"There are\", CME_data.shape[0], \"flares with associated CMEs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download new dataset from:\n",
    "# dataset= 'https://github.com/JasonTLWang/RNN-CME-prediction/blob/master/CME_data_samples/normalized_testing_24.csv'\n",
    "# df = pd.read_csv('cycle24/normalized_testing_24.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the data\n",
    "print(\"Number of features:\", len(CME_data[0]))\n",
    "\n",
    "# positive class\n",
    "print(\"Positive class:\")\n",
    "print(\"Number of events:\", len(CME_data))\n",
    "\n",
    "# negative class\n",
    "print(\"Negative class:\")\n",
    "print(\"Number of events:\", len(no_CME_data))\n",
    "\n",
    "# Ratio: negative class to positive class\n",
    "ratio = len(no_CME_data)/len(CME_data)\n",
    "print(\"Ratio of negative class to positive class:\", ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will later investigate another way to get a balanced dataset. We will use the `imbalanced-learn` library to generate synthetic samples of the minority class and see the impact on the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_smote = False\n",
    "if run_smote:\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    # Apply the SMOTE algorithm to balance the dataset\n",
    "    sm = SMOTE(random_state=42)\n",
    "\n",
    "    xsample = np.concatenate((CME_data, no_CME_data), axis=0)\n",
    "    ysample = np.concatenate((np.ones(len(CME_data)), np.zeros(len(no_CME_data))))\n",
    "\n",
    "    xresampled, yresampled = sm.fit_resample(xsample, ysample)\n",
    "\n",
    "    # Summary of the data\n",
    "    print(\"Number of features:\", len(xresampled[0]))\n",
    "    print(\"Number of events:\", len(xresampled[yresampled == 1]))\n",
    "    print(\"Number of no events:\", len(xresampled[yresampled == 0]))\n",
    "\n",
    "\n",
    "    CME_data = xresampled[yresampled == 1]\n",
    "    no_CME_data = xresampled[yresampled == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature selection (filter methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step before starting to use any predictive method is to study the information present in the data. To do this, we are going to study the distribution of the characteristics of the active regions that produced a CME and those that did not. We will see the distribution of a feature for the active regions that produced a CME (green) and for the active regions that did not produce a CME (red). You can change the value of `i` in the code block below to see that some features are not at all useful since there is hardly any difference in the distributions for the positive and negative class. Therefore, we could discard those features from our sample before starting the modeling.\n",
    "\n",
    "This exercise is commonly referred to as Feature filtering and is based on the statistical properties of the features, such as correlation, variance, etc. These methods are fast and scalable, but do not consider the interactions between features or the predictive power of the subset of features. Other feature selection methods will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharps = ['Total unsigned flux', 'Mean gradient of total field',\n",
    "          'Mean current helicity (Bz contribution)', 'Mean photospheric magnetic free energy',\n",
    "          'Fraction of Area with Shear > 45 deg', 'Total unsigned current helicity',\n",
    "          'Mean gradient of horizontal field', 'Mean characteristic twist parameter, alpha',\n",
    "          'Mean angle of field from radial', 'Mean gradient of vertical field',\n",
    "          'Mean vertical current density', 'Total unsigned vertical current',\n",
    "          'Sum of the modulus of the net current per polarity',\n",
    "          'Total photospheric magnetic free energy density', 'Mean shear angle',\n",
    "          'Area of strong field pixels in the active region', 'Sum of flux near polarity inversion line',\n",
    "          'Absolute value of the net current helicity']\n",
    "\n",
    "sharp_mini = ['USFLUX', 'MEANGBT', 'MEANJZH', 'MEANPOT', 'SHRGT45', 'TOTUSJH', 'MEANGBH', 'MEANALP', 'MEANGAM', 'MEANGBZ', 'MEANJZD', 'TOTUSJZ', 'SAVNCPP', 'TOTPOT', 'MEANSHR', 'AREA_ACR', 'R_VALUE', 'ABSNJZH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# For the positive class (green)\n",
    "mu_fl = np.mean(CME_data[:, i])\n",
    "sigma_fl = np.std(CME_data[:, i])\n",
    "num_bins = 15\n",
    "n_fl, bins_fl, patches_fl = plt.hist(CME_data[:, i], num_bins, facecolor='C2', alpha=0.5,density=True)\n",
    "y_fl = scipy.stats.norm.pdf(bins_fl, mu_fl, sigma_fl)\n",
    "plt.plot(bins_fl, y_fl, 'C2--', label='positive class')\n",
    "\n",
    "# For the negative class (red)\n",
    "mu_nofl = np.mean(no_CME_data[:, i])\n",
    "sigma_nofl = np.std(no_CME_data[:, i])\n",
    "n_nofl, bins_nofl, patches_nofl = plt.hist(no_CME_data[:, i], num_bins, facecolor='C3', alpha=0.5,density=True)\n",
    "y_nofl = scipy.stats.norm.pdf(bins_nofl, mu_nofl, sigma_nofl)\n",
    "plt.plot(bins_nofl, y_nofl, 'C3--', label='negative class')\n",
    "\n",
    "plt.xlabel('Normalized '+sharps[i], fontsize=12)\n",
    "plt.ylabel('Number (normalized)', fontsize=12)\n",
    "plt.minorticks_on()\n",
    "plt.locator_params(axis='y', nbins=6)\n",
    "legend = plt.legend(loc='upper right', fontsize=12, framealpha=0.0, title='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the Univariate F-score for feature selection. It is a very simple method: the F-score measures the distance between the two distributions for a given feature (inter-class distance), divided by the sum of the variances for this feature (intra-class distance). We can use the `sklearn.feature_selection` module to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the feature selection method\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# select the number of features\n",
    "N_features = CME_data.shape[1]\n",
    "Nfl = CME_data.shape[0]\n",
    "Nnofl = no_CME_data.shape[0]\n",
    "yfl = np.ones(Nfl)\n",
    "ynofl = np.zeros(Nnofl)\n",
    "# k is the number of features\n",
    "selector = SelectKBest(f_classif, k=N_features)\n",
    "selector.fit(np.concatenate((CME_data, no_CME_data), axis=0),\n",
    "             np.concatenate((yfl, ynofl), axis=0))\n",
    "scores = selector.scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "order = np.argsort(selector.scores_)\n",
    "orderedsharps = [sharps[i] for i in order]\n",
    "orderedscores = [selector.scores_[i]/np.max(selector.scores_) for i in order]\n",
    "y_pos2 = np.arange(18)\n",
    "bars = plt.barh(y_pos2, orderedscores, color='C3', alpha=0.8, height=0.8)\n",
    "plt.yticks(y_pos2, orderedsharps, fontsize=12)\n",
    "plt.xlabel('Normalized Fisher Score', fontsize=12)\n",
    "ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False) \n",
    "ax.xaxis.set_visible(False)\n",
    "\n",
    "def custom_fmt(x):\n",
    "    return '<0.01' if x < 0.01 else '%.2f' % x\n",
    "\n",
    "ax.bar_label(bars, padding=+5, color='C3', \n",
    "             fontsize=12, label_type='edge', fmt=custom_fmt,\n",
    "             fontweight='bold')\n",
    "# Add title:\n",
    "plt.title('Normalized Fisher Score for each feature')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the support vector machine on the data. The SVM uses non-linear decision functions to map the feature space into a higher-dimensional space, where the positive and negative examples can be separated linearly by a hyperplane. <br>\n",
    "\n",
    "This is incredibly non-intuitive. But we can think of a simpler example. Suppose we had two classes: CME-producing and non-CME producing active regions. And suppose we had two features: the total flux in these regions, and the total area of these regions. We could construct a two-dimentional feature space, where we plot the flux against the area of each active region. Positive examples could be indicated by an X and negatives ones by an O. In theory, if our data behaved well, we could draw a line between these classess. <br>\n",
    "\n",
    "Since we have 18 features, the SVM constructs an 18-dimensional feature space. In this feature space, the decision boundary separating the positive and negative examples may be non-linear. As such, the algorithm then enlarges this 18-dimensional feature space (using the function indicated by the `kernel` parameter in the [`svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) function) into a higher-dimensional feature space wherein it is possible to linearly separate the positive and negatives classes. There are lots of people trying to work on how to [visualize these multi-dimensional feature spaces](https://github.com/tmadl/highdimensional-decision-boundary-plot), which is an active area of research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = Nfl + Nnofl\n",
    "\n",
    "\n",
    "C = 1e+0\n",
    "gamma = 1e-1\n",
    "clf = svm.SVC(C=C, gamma=gamma, kernel='rbf', class_weight='balanced',\n",
    "              cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stratified k-folds cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run and evaluate the performance of the SVM. There are lots of different ways to evaluate the performance of a classifier, which we discuss in Section 4 of [Bobra & Couvidat (2015)](https://arxiv.org/abs/1411.1405). We're going to choose a metric called the True Skill Score, or the TSS, which we can calculate from four quantities: true positives, true negatives, false positives, and false negatives. We prefer the TSS to all the other metrics as it is insensitive to the class imbalance ratio and thus best for comparison to other groups. The TSS is symmetrically distributed about 0: i.e., it goes from [-1, 1] where 0 represents no skill and a negative value represents a perverse prediction. Thus we are able to predict CMEs in a fashion better than randomly guessing. Here we define a confusion table to measure the performance of our binary classification: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_table(pred, labels):\n",
    "    \"\"\"\n",
    "    computes the number of TP, TN, FP, FN events given the arrays with predictions and true labels\n",
    "    and returns the true skill score\n",
    "\n",
    "    Args:\n",
    "    pred: np array with predictions (1 for flare, 0 for nonflare)\n",
    "    labels: np array with true labels (1 for flare, 0 for nonflare)\n",
    "\n",
    "    Returns: true negative, false positive, true positive, false negative\n",
    "    \"\"\"\n",
    "    Nobs = len(pred)\n",
    "    TN = 0.\n",
    "    TP = 0.\n",
    "    FP = 0.\n",
    "    FN = 0.\n",
    "    for i in range(Nobs):\n",
    "        if (pred[i] == 0 and labels[i] == 0):\n",
    "            TN += 1\n",
    "        elif (pred[i] == 1 and labels[i] == 0):\n",
    "            FP += 1\n",
    "        elif (pred[i] == 1 and labels[i] == 1):\n",
    "            TP += 1\n",
    "        elif (pred[i] == 0 and labels[i] == 1):\n",
    "            FN += 1\n",
    "        else:\n",
    "            print(\"Error! Observation could not be classified.\")\n",
    "    return TN, FP, TP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the SVM on our data and cross-validate our results. In our case, the positive sample size is quite small (both objectively and compared to the negative sample size). Therefore, we use a stratified k-folds cross-validation method, which makes k partitions of the data set and uses k-1 folds for training the SVM and 1 fold for testing the trained SVM. The stratification preserves the ratio of positive to negative examples per fold. Then we can permute over the partitions such that each partition eventually makes its way into the testing set. For each individual testing set, we can calculate a skill score. Then we can average the skill scores over the total number of testing sets. \n",
    "\n",
    "To compute the TSS, we must first select a value of k. k can be arbitrarily defined and take any value between 2 and `number_of_examples`, so we can explore this parameter space. As k approaches `number_of_examples`, the k-fold method reduces to the Leave One Out method, in which only one example is in the testing set and all other examples are in the training set. The literature suggests this method is not the best, so we can stray away from high values of k. Many studies (e.g. [Kohavi, 1995](http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf)) recommend the stratified 10-fold cross-validation to reduce variance and bias. Here, we test their recommendation by computing the TSS using 30 k values, ranging from 2 to 32. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the mean TSS per k, using the standard deviation as the error in the TSS. We see that for high values of k, the standard deviation in the TSS can be greater than the mean. These points are indicated in blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we confirm that high k-values result in a high variance. We find it reasonable to use the stratified 10-fold cross-validation method to compute the TSS and will follow this recommendation. Therefore we report this score as our final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to hold the TSS and standard deviation of the TSS\n",
    "\n",
    "\n",
    "# xdata are the examples\n",
    "# ydata are the labels\n",
    "xdata = np.concatenate((CME_data, no_CME_data), axis=0)\n",
    "ydata = np.concatenate((np.ones(Nfl), np.zeros(Nnofl)), axis=0)\n",
    "mdata = np.concatenate((positive_class, negative_class), axis=0)\n",
    "\n",
    "# # # Create fake dataset:\n",
    "# shuffle_index = np.random.permutation(ydata.shape[0])\n",
    "# ydata = ydata[shuffle_index]\n",
    "\n",
    "\n",
    "k = 4 # 25% of the data is used for testing\n",
    "# skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "skf = RepeatedStratifiedKFold(n_splits=k, n_repeats=10)\n",
    "these_TSS_for_this_k = []\n",
    "confusion_matrix_k = []\n",
    "for train_index, test_index in skf.split(xdata, ydata):\n",
    "    # xtrain are the examples in the training set\n",
    "    xtrain = xdata[train_index]\n",
    "    # ytrain are the labels in the training set\n",
    "    ytrain = ydata[train_index]\n",
    "    # xtest are the examples in the testing set\n",
    "    xtest = xdata[test_index]\n",
    "    # ytest are the labels in the testing set\n",
    "    ytest = ydata[test_index]    \n",
    "    # # metadata useful for interpreting with LIME\n",
    "    # mtrain = mdata[train_index]\n",
    "    # # metadata useful for interpreting with LIME\n",
    "    # mtest = mdata[test_index]\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "    if (((TP+FN) == 0.0) or (FP+TN) == 0.0):\n",
    "        these_TSS_for_this_k.append(np.nan)\n",
    "        continue\n",
    "    else:\n",
    "        these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "        confusion_matrix_k.append([TN, FP, TP, FN])\n",
    "    # break\n",
    "confusion_matrix_ = np.mean(confusion_matrix_k, axis=0)\n",
    "TSS_k = np.array(these_TSS_for_this_k)\n",
    "\n",
    "print(confusion_matrix_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, TP, FN = confusion_matrix_\n",
    "confusion_matrix = np.array([TP, FN, FP, TN]).reshape(2, 2)\n",
    "print(TP/(TP+FN) - FP/(FP+TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=['CME','No CME'])\n",
    "display.plot(cmap='Blues')\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "# Now normalize the confusion matrix\n",
    "display = ConfusionMatrixDisplay(confusion_matrix= (confusion_matrix.T / np.sum(confusion_matrix,axis=1)).T, display_labels=['CME','No CME'])\n",
    "display.plot(cmap='Blues')\n",
    "plt.title('Normalized confusion matrix')\n",
    "for im in plt.gca().get_images():                   # set clim manually within the image\n",
    "    im.set_clim(vmin=0,vmax=1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSS\n",
    "TSS_tun_list = []\n",
    "\n",
    "# Train the model\n",
    "param_grid = {'C': 10**np.arange(-2, 3, 1.0),\n",
    "              'gamma': 10**np.arange(-2, 3, 1.0)}\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "skf = RepeatedStratifiedKFold(n_splits=4, n_repeats=5)\n",
    "for CC in param_grid['C']:\n",
    "    for GG in param_grid['gamma']:\n",
    "        clf = svm.SVC(C=CC, gamma=GG, kernel='rbf', class_weight='balanced',\n",
    "                      cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n",
    "        these_TSS_for_this_k = []\n",
    "        for train_index, test_index in skf.split(xdata, ydata):\n",
    "            xtrain = xdata[train_index]\n",
    "            ytrain = ydata[train_index]\n",
    "            xtest = xdata[test_index]\n",
    "            ytest = ydata[test_index]\n",
    "            clf.fit(xtrain, ytrain)\n",
    "            TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "            if (((TP+FN) == 0.0) or (FP+TN) == 0.0):\n",
    "                these_TSS_for_this_k.append(np.nan)\n",
    "                continue\n",
    "            else:\n",
    "                these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "        TSS_k = np.array(these_TSS_for_this_k)\n",
    "        TSS_tun_list.append(np.mean(TSS_k))\n",
    "\n",
    "TSS_tuned = np.array(TSS_tun_list).reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=0.15, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(TSS_tuned, interpolation='nearest', cmap='CMRmap',vmin=None, vmax=1.0)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.xticks(np.arange(len(param_grid['gamma'])), ['%.1e' % i for i in param_grid['gamma']])\n",
    "plt.yticks(np.arange(len(param_grid['C'])), ['%.1e' % i for i in param_grid['C']])\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('TSS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_2d_range = [1e-2, 1, 1e1]\n",
    "gamma_2d_range = [1e-2, 1, 1e1]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf_ = svm.SVC(C=C, gamma=gamma, kernel='rbf', class_weight='balanced',\n",
    "                cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)\n",
    "        clf_.fit(xdata[:, :2], ydata)\n",
    "        classifiers.append((C, gamma, clf_))\n",
    "        \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 5, 200), np.linspace(-4, 5, 200))\n",
    "for k, (C, gamma, clf_) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    Z = clf_.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "    plt.title(r\"gamma=$10^{%d}$, C=$10^{%d}$\" % (np.log10(gamma), np.log10(C)), size=\"medium\")\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(xdata[:, 0], xdata[:, 1], c=ydata, cmap=plt.cm.RdBu_r, s=1)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.axis(\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Feature selection (wrapper methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use a Recursive Feature Addition algorithm. This algorithm adds features one by one and checks the TSS. We will add the feature which increases the TSS the most. This strategy does not guarantee always the best solution (as we are not checking all possible combinations), but it is fast and gives good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = 1e+0\n",
    "best_gamma = 1e-1\n",
    "print(\"The best combination of C and gamma is\", best_C, \"and\", best_gamma)\n",
    "\n",
    "\n",
    "clf = svm.SVC(C=best_C, gamma=best_gamma, kernel='rbf', class_weight='balanced',\n",
    "                cache_size=500, max_iter=-1, shrinking=True, tol=1e-8, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sequential feature selection: adding features one by one\n",
    "\n",
    "total_features = CME_data.shape[1]\n",
    "\n",
    "best_combination = []\n",
    "TSS_for_best_combination = []\n",
    "for combination in range(1, total_features+1):\n",
    "    TSS_for_this_combination = []\n",
    "    # features_to_use is the index saved in best_combination and the index of the feature we are testing\n",
    "    for feature_to_test in range(total_features):\n",
    "        if feature_to_test in best_combination:\n",
    "            TSS_for_this_combination.append(0.0)\n",
    "            continue\n",
    "        features_to_use = best_combination.copy()\n",
    "        features_to_use.append(feature_to_test)\n",
    "\n",
    "        # skf = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "        skf = RepeatedStratifiedKFold(n_splits=4, n_repeats=2)\n",
    "        these_TSS_for_this_k = []\n",
    "        for train_index, test_index in skf.split(xdata, ydata):\n",
    "            xtrain = xdata[train_index][:, features_to_use]\n",
    "            ytrain = ydata[train_index]\n",
    "            xtest = xdata[test_index][:, features_to_use]        \n",
    "            # mtest = mdata[test_index]\n",
    "            ytest = ydata[test_index]\n",
    "            clf.fit(xtrain, ytrain)\n",
    "            TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "            these_TSS_for_this_k.append(TP/(TP+FN) - FP/(FP+TN))\n",
    "        TSS = np.mean(these_TSS_for_this_k)\n",
    "        \n",
    "        TSS_for_this_combination.append(TSS)\n",
    "    # print('All TSS for this combination:', TSS_for_this_combination)\n",
    "    \n",
    "    best_combination.append(np.argmax(TSS_for_this_combination))\n",
    "    TSS_for_best_combination.append(np.max(TSS_for_this_combination))\n",
    "    print(\"The best combination so far is\", best_combination, \"with a TSS of\", \"{:.2f}\".format(np.max(TSS_for_this_combination)), \n",
    "          \"where addition had a score of\", \"{:.2f}\".format(selector.scores_[np.argmax(TSS_for_this_combination)]/np.max(selector.scores_)))\n",
    "\n",
    "    # break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best combination of features with the names:\n",
    "best_combination_names = [sharps[i] for i in best_combination]\n",
    "for i in range(len(best_combination)):\n",
    "    print(\"Feature\", best_combination[i], \":\", best_combination_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important trend we can already see is that as we start adding too many features, the TSS starts to decrease.\n",
    "plt.plot(np.arange(1, total_features+1), TSS_for_best_combination, 'o-', color='C4')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('TSS')\n",
    "plt.title('TSS as a function of the number of features')\n",
    "plt.xticks(np.arange(1, total_features+1))\n",
    "plt.ylim(ymax=1.0, ymin=0)\n",
    "plt.locator_params(axis='x', nbins=9)\n",
    "plt.minorticks_on()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Index of the features that are selected\n",
    "label_of_features = []\n",
    "for feature in range(18):\n",
    "    label_of_features.append(best_combination.index(feature))\n",
    "\n",
    "norder = np.argsort(label_of_features)[::-1]\n",
    "norderedsharps = [sharps[i] for i in norder]\n",
    "nscores = [selector.scores_[i] for i in norder]\n",
    "y_pos2 = np.arange(18)\n",
    "bars = plt.barh(y_pos2, nscores/np.max(scores), color='C3', alpha=0.8, height=0.8)\n",
    "plt.yticks(y_pos2, norderedsharps, fontsize=12)\n",
    "plt.xlabel('Normalized Fisher Score', fontsize=12)\n",
    "ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False) \n",
    "ax.xaxis.set_visible(False)\n",
    "\n",
    "plt.title('Normalized Fisher Score, ordered by feature selection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Agnostic \"local\" Explanations (Shapley values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical train-test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.25, random_state=42)\n",
    "clf.fit(xtrain, ytrain)\n",
    "TN, FP, TP, FN = confusion_table(clf.predict(xtest), ytest)\n",
    "print([TN, FP, TP, FN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.KernelExplainer(clf.predict_proba, shap.kmeans(xdata, 200))\n",
    "i = np.random.randint(0, xtest.shape[0])\n",
    "shap_values = explainer.shap_values(xtest[i, :])\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[:, 0], xtest[i, :], feature_names=sharps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the event in the metadata:\n",
    "index_event = np.where(np.all(xdata == xtest[i, :], axis=1))[0][0]\n",
    "print(\"The event was a\", mdata[index_event][2], \"class event, in the AR number\", mdata[index_event][1], \"at time\", mdata[index_event][3], \"with a CME associated\" if ydata[index_event] == 1 else \"with no CME associated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a timedelay of 24 hours to predict CMEs. If the number is much smaller, we will probably have better changes of predicting CMEs as the magnetic field just before the event could problably a better signature of complexity of the region and help us to predict the CMEs. So there are many things to explore in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the predictor to analyze new observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal check\n",
    "# import datetime\n",
    "\n",
    "# for ii in range(selected_metadata.shape[0]):\n",
    "#     print(\"Event at AR\", selected_metadata[ii,1], selected_metadata[ii,2], (datetime.datetime.strptime(selected_metadata[ii,3], '%Y.%m.%d_%H:%M_TAI') + datetime.timedelta(hours=timedelayvariable)).strftime('%Y-%m-%dT%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_index = 10\n",
    "\n",
    "my_features = selected_features[student_index]\n",
    "my_metadata = selected_metadata[student_index]\n",
    "\n",
    "# Now it your turn to investigate your AR:\n",
    "print(\"Investigating NOAA Active Region\", my_metadata[1], \"(HARPNUM\", my_metadata[0], \")\")\n",
    "\n",
    "# Good luck with your analysis!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model prediction will colapse the output to the 0 or 1 class\n",
    "my_prediction = clf.predict(my_features[None, :])\n",
    "print('This AR belongs to the class:', my_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can use probabilities to get a more detailed prediction\n",
    "print(\"My model predicts a CME with a probability of\", clf.predict_proba(my_features[None, :])[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check in Jhelioviewer if there is something at the time of the event:\n",
    "print(\"Event at AR\", selected_metadata[student_index,1], selected_metadata[student_index,2], (datetime.datetime.strptime(selected_metadata[ii,3], '%Y.%m.%d_%H:%M_TAI') + datetime.timedelta(hours=timedelayvariable)).strftime('%Y-%m-%dT%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
